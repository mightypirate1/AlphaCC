services:
  trainer:
    image: alphacc/main
    build:
      dockerfile: Dockerfile.main
      target: main
    shm_size: '8gb'
    cpu_shares: 4096
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      keydb-main:
        condition: service_healthy
      keydb-pred:
        condition: service_started
    volumes:
      - local:/mnt:rw
    env_file:
      - .env.docker
    command: [
      "alphacc-trainer",
      "--run-id=lesgo-size9-keydb+memcache-0-nsamples=3000",
      "--size=9",
      "--tournament-freq=20",
      "--n-train-samples=3000",
      "--replay-buffer-size=30000",
      "--train-size=15000",
      "--epochs-per-update=3",
      "--policy-weight=1.0",
      "--value-weight=1.0",
      "--entropy-weight=0.01",
      "--l2-reg=3e-4",
      "--batch-size=128",
      "--lr=1e-4",
      "--gpu",
    ]

  worker:
    image: alphacc/main
    #cpu_shares: 256
    deploy:
      replicas: 50
    depends_on:
      - nn-service
    env_file:
      - .env.docker
    restart: always
    command: [
      "alphacc-worker",
      "--size=9",
      "--n-rollouts=init=100,step=4,min=0,max=3000",
      "--rollout-depth=init=10,step=1,max=100",
      "--action-temperature=init=1.3,step=-0.0015,min=0.01,max=1.0",
      "--max-game-length=init=250,random_add=100",
      "--dirichlet-noise-weight=0.15",
      "--rollout-gamma=0.99",
    ]

  nn-service:
    image: alphacc/main
    cpu_shares: 8192
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - trainer
    env_file:
      - .env.docker
    restart: always
    command: [
      "alphacc-nn-service",
      "--size=9",
      "--inference-batch-size=256",
      "--gpu",
    ]

  tensorboard:
    image: tensorflow/tensorflow
    network_mode: host
    depends_on:
      - trainer
    volumes:
      - local:/mnt:ro
    command:
      - "tensorboard"
      - "--logdir=/mnt/logdir"

  keydb-main:
    image: eqalpha/keydb
    cpu_shares: 256
    ports:
      - '6379:6379'
    command: [
      "keydb-server",
      "--appendonly", "no",
      "--save", "",
      "--server-threads", "2",
      "--tcp-backlog", "511",
      "--tcp-keepalive", "60",
      "--client-output-buffer-limit", "normal", "0", "0", "0",
      "--client-output-buffer-limit", "pubsub", "32mb", "8mb", "60",
    ]
    healthcheck:
      test: [ "CMD", "redis-cli", "--raw", "incr", "ping" ]
      interval: 5s
      timeout: 30s
      retries: 15
      start_period: 5s

  keydb-pred:
    extends:
      file: docker/pred-db.yaml
      service: keydb-pred

  memcached:
    image: memcached:latest
    cpu_shares: 8192
    ports:
      - "11211:11211"
    command: [
      "-m", "1024",
      "-c", "256",
      "-t", "24",
      "-I", "4k",
      "-n", "48",
      "-f", "1.1",
      "-o", "slab_reassign,slab_automove=1,modern,slab_chunk_max=4",
    ]
    restart: unless-stopped

volumes:
  local:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "./data"
